\chapter{Proceso de aprendizaje}

Tras las descripción de los modelos a utilizar, y las funciones de pérdida a emplear, comienza el proceso de entrenamiento de los modelos. Este proceso es uno de los más lentos de todo el desarrollo del proyecto, debido a la cantidad de horas necesarias a esperar para recopilar los resultados del modelo. Se estudiará la eficiacia de cada modelo a la hora de evaluar el problema, y las decisiones tomadas para su mejora y elección.\\

Para resolver este problema, se ha tomado un enfoque de dos niveles; en primer lugar, distinguiremos si la enfermedad que se recibe como entrada se trata de un ejemplar de enfermedad maligna o benigna, y posteriormente, su salida indicará un segundo modelo a aplicar sobre la enfermedad, especializado únicamente en uno de los dos tipos de enfermedades. Así, conseguimos un conjunto de modelos especialziados que permiten otorgar el diagnóstico con mayor precisión y seguridad.\\

Un aspecto clave de este proceso será la eficiencia, pero sobre todo el correcto diagnóstico de las enfermedades, por lo que usaremos como métricas de selección la precisión, el recall y el accuracy balanceado:

\begin{itemize}
	\item Precision = TP / (TP + FP). Proporción de valores bien clasificados dentro de una clase teniendo en cuenta verdaderos y falsos positivos.
	\item Recall = TP / P. Valores correctamente identificados como positivos respecto al total de elementos positivos.
	\item Accuracy balanceado: combina sensitivity (TP/(TP+FN)) y specificity (TN/(TN+FP)). La primera, devuelve el valor real de proporción de valores correctamente clasificados entre el total de casos positivos que tenemos (contando predicciones positivas y falsos negativos), mientras que la especifidad, nide el caso dual: la proporción de casos negativos bien clasificados respecto al total de datos negativos tanto bien clasificados como mal clasificados, y que son identificados como falsos positivos. Esto nos permite calcular el accuracy de forma proporcional al porcentaje de presencia de las clases, e intentamos así paliar el efecto del desequilibrio de los datos:
	
$$ Balanced\ accuracy: \frac{Sensitivity + Specificity}{2}$$
	
	En base a esta expresión, podemos saber que si el valor es aproximadamente 1/númeroClases, es posible que gran parte de las clases no estén siendo correctamente clasificadas en caso de ser minoritarias. Valores mayores nos harán conocer que el resultado de clasificar todas las clases es satisfactorio.
\end{itemize}

\section{Clasificación binaria}

El problema de la clasificación binaria consiste en la distinción de las enfermedades malignas de aquellas que son benignas. Se trata del primer problema a resolver antes de especificar el tipo de enfermedad que posee el paciente, y así poder especializar los modelos segundo nivel.

\subsection{Equilibrio de los datos}

El primer inconveniente que nos encontramos para este problema es el inmenso desbalaceo entre la clase benigna y maligna. Si redibujamos el gráfico mostrado durante la fase de preprocesamiento de datos, únicamente con los datos de entrenamiento, obtenemos lo siguiente: 

\begin{figure}[H]
	\centering
	\label {desequilibriototal}
	\includegraphics[scale = 0.4]{imagenes/desequilibriototal.png}
	\caption{Desequilibrio de datos}
\end{figure}

Se puede observar la alta disparidad existente entre ambas, siendo 35664 casos benignos y 9752 malignos. Es una diferencia de 3.65 veces más casos benignos que malignos, lo que se traduce en un equilibrio de  79\% y 21\% respectivamente. Para paliarlo, podemos efectuar dos estrategias: aplicar técnicas de oversampling (también conocido como sobremuestreo sintético), donwsampling de la clase mayoritaria, o bien, la aplicación de penalziaciones sobre la clase minoritaria para que su incorrecta clasificación tenga mayores consecuencias (haciendo uso de FocalLoss).\\

\subsubsection{Técnicas descartadas}

La ténica de downsampling de la clase mayoritaria queda descartada, debido a que dentro de esa clase, podemos encontrar etiquetas de segundo nivel asociadas a las enfermedades que se pueden diagnosticar. Como algunas de estas clases son muy escasas en número, si realizamos downsampling de forma aleatoria sin ningún tipo de restriccióin, podría darse el caso límite en la que una clase completa desapareciese del cojunto. Y esto conllevaría a la obtención de errores al evaluación de test, pues tendríamos una clase presente en test no estudiada en entrenamiento, que sólo provocaría un aumento de las métricas de error. \\

El uso de la función de pérdida FocalLoss podría ayudar a paliar este efecto. Sin embargo, debido al desajuste 79-21 del que disponemos, y la gran variedad de imágenes, se trata de una solución demasiado arriesgada. Para comprobarlo, se sometíó a evaluación con los valores por defecto recomendados de Resnet50. A priori, los resultados podrían parecer correctos en validación \ref{tab:resultsfl}; sin embargo, observamos que la clase minoritaria, la maligna, obtiene resultados que no están a la altura de la clase mayoritaria. Observamos un valor 0.41 en recall, valor preocupante frente al 0.95 de la clase benigna. Al existir un recall bajo, quiere decir que esta clase no se está detectando adecuadamente como maligna, y casi el 60\% de sus casos son calificados como benignos. Esto es un grave problema, ya que los casos malignos son los que más riesgo conllevan para la salud. Por tanto, se descarta también esta estrategia.


\begin{table}[!ht]
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		& Precision & Recall & F1-score & Support \\
		\hline
		Benign & 0.86 & 0.95 & 0.90 & 15338 \\
		Malignant & 0.68 & 0.41 & 0.51 & 4126 \\
		\hline
		Accuracy &  &  & 0.83 & 19464 \\
		Macro avg & 0.77& 0.68& 0.71&19464\\
		Weighted avg&0.82&0.83&0.82&19464\\
		\hline
	\end{tabular}
	\caption{Informe de clasificación}
	\label{tab:resultsfl}
\end{table}

\subsubsection{Propuesta: oversampling}

Sólo nos queda disponible la estategia de oversampling: la ampliación sintética de la clase minoritaria. Este proceso es muy delicado, ya que se basa en realizar alteraciones sobre las imágenes disponibles para dotarlas al conjunto de una mayor variabilidad, hasta equiparar el número de ejemplares con el de la clase mayoritaria. Existen distintas técnicas a aplicar, pero en este caso, utilizaré la estrategia de realizar alteraciones sobre las imágenes mediante una biblioteca de transformaciones: imgaug.

Se aplicarán una serie de transformaciones la imagen que alteren su estructura, pero con cuidado de no generar imágenes demasiado modificadas que se alejen de la distribución original de la clase maligna. Para realizar las transformaciones, he contruido un pipeline, una secuencia de transformaciones que aplica, en orden aleatorio, y con cierta probabilidad, las siguientes variaciones:

\begin{itemize}
	\item Fliplr(p=0.5). Se trata de un efecto de espejo de la imagen, respecto a su eje horizontal. Es equivalente a realizar una rotación de la imagen de 180º sobre su eje teórico horizontal que transcurre por la mitad de la imagen. Esta alteración se realziará con un 50\% de probabilidad.
	\item Flipud(p=0.5). Se trata de un efecto de espejo de la imagen, respecto a su eje vertical. Es un caso de efectos similares al anterior, aplicado con un 50\% de probabilidad.
	\item Gaussian blur (p=0.5). Se aplicará filtro gaussiano en el 50\% de los casos, especificándose un valor de sigma muy pequeño de 0.5.
	\item Modificación de contraste  (p=0.25). Se aplicará un aumento o decremento del contraste del 20\% en el 25\% de los casos. Esto emula posibles cambios de luminosidad ambiente y de profundidad de color de la cámara
	\item Ruido Gaussiano aditivo  (p=0.5). Se aplica en caso de que el ruido gaussiano estándar no sea aplicado. Aplica un filtro gaussiano a nivel de canal, de forma que solo parte de la imagen se encuentra difuminada.  Se aplica también en un 50\% de los casos
	\item Transformaciones afines  (p=0.2): se trata de una alteración compuesta de escala y zoom, y una posterior traslación o rotación de la imagen. Se trata de una opcionalidad ofrecida por imgaug de forma no configurable directamente.
\end{itemize}

Estos valores han sido escogidos tomando como inspiración las transformaciones realizadas por el equipo ganador de la competición de ISIC \cite{1stISIC}, debido a sus buenos resultados.

Es importante destacar que dichas transformaciones únicamente se han realizado sobre los datos de entrenamiento; los datos de validación y test permanecen inalterados de forma que éstos sigan siendo un estimador no sesgado del rendimiento del modelo. Poir tanto, la extracción del 30\% de validación se realiza antes del sobremuestreom y los dos subconjuntos se guardan en carpetas distintas, a las que llamaremos train y test.

\subsection{Construcción del modelo}

Con los datos preparados, configurar mediante Fastai y Pytorch los modelos que deseamos probar. Los 3 modelos siguen la misma configuración, a excepción de la importación del modelo, por lo  que será explicado de forma paramétrica. Tenemos 3 tareas diferenciables: la creación del datablock para la gestión de los datos, el entrenamiento del modelo, y la transformación del mismo al modelo cuantizado empleado en el dispositivo Android.

\subsubsection{Creación del datablock}

FastAI ofrece el tipo de objeto Datablock para la creación de una estructura que organiza los datos de entrenamiento. Permite construir un bloque con todos los datos, de forma que cualquier consulta o verificación pueda realizarse sobre un mismo objeto. Consta de varios parámetros que podemos ajustar para normalizar las imágenes, adaptar el tamaño de entrada, o realizar la separación train-validación. Desglosadando los parámetros más importantes, tenemos:

\begin{itemize}
	\item blocks. Nos permite establecer el formato de entrada de datos, y la salida que queremos obtener de los mismos. En problemas de clasificación, habitualmente, tendremos como entrada un bloque de imágenes, y como salida, queremos obtener un conjunto de categorías asociados al bloque.
	\item get\_items. Establece la fuente de la cual obtener los datos para la construcción del DataBlock. Se trata de una función preconfigurada para la lectura de datos dado un repositorio o dirección de disco. En nuestro caso, como las imágenes no contienen información compleja, como plantillas de segmentación, se importan directamente de esta forma.
	\item splitter. Permite especificar divisiones de los datos. Habitualmente, se utiliza la función RandomSplitter, que nos permite separar los datos en entrenamiento y validación. Como parámetros de entrada, recibe el porcentaje de datos para validación que queremos obtener, y opcionalmente una semilla, por si deseamos repetibilidad de los experimentos. Como en este caso, queremos que el conjunto de entrenamiento sea el conjunto sobremuestreado, y validación, sea la carpeta valid, se ha creado una función que indica 1 si la imagen pertenece a la carpeta valid, y 0, si se encuentra en train. \ref{fig:particiones}
	\item get\_y. Parámetro al cual se asocian las etiquetas de los elementos de entrada. Puede recibir una función o bien una lista con las etiquetas asociadas a los elementos del dataset. FastAI toma las etiquetas por defecto del nombre de carpeta, por lo que tendremos que especificarle una función que tome la etiqueta del dataset \ref{fig:etiquetadobinario}.
	\item item\_tfms. Establece modificaciones sobre las imágenes de forma previa a la ejecución del modelo. En este caso, como ya hemos realizado oversampling, no realizaremos ninguna otra modificación sobre las imágenes para no transformar en exceso los datos.
	\item batch\_tfms. Permite realizar transformaciones a nivel de batch. Permite aplicar operaciones como la normalización de las imágenes. Este procedimiento favorece la convergencia del modelo, aunque el cálculo de la media y desviación típica es costoso. En su lugar, utilizaré la normalización que provee ImageNet, donde ya se incluyen los valores estadísticos de media y desviación para los datos en la variable *image\_stats provenientes de decenas de millones de imágenes.
	
\end{itemize}


 \begin{algorithm}[H]
	\caption{Función para la distinción de entrenamiento y validación}
	\label{fig:particiones}
	\begin{algorithmic}[1]
		
		\Procedure{ val\_splitter}{fname}
		\Comment{Comprueba si la carpeta contenedora es validiación}
		\State var pertenece: boolean = False
		\If{Path(fname).parent.name = valid}
			\State pertenece = True
		\EndIf \\
		\Return pertenece
		\EndProcedure
		
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Función para la consulta de etiquetas}
	\label{fig:etiquetadobinario}
	\begin{algorithmic}[1]
		
		\Procedure{ binary\_label}{fname, df\_data: Dataframe}
		\State coincidence = $df\_data[i]$ where $df\_data[i] = fname, i \in Integer$
		\State var label = coincidence.label\\
		\Return label
		\EndProcedure
		
	\end{algorithmic}
\end{algorithm}

Una vez definido, podremos instanciar el datablock, configurando uno de los parámetros más críticos del entrenamiento: el tamaño de batch.

Cuando entrenamos una red, estamos ajustando una serie de pesos según recorremos los datos. Éstos, por tanto, pueden actualizarse en diferentes momentos: tras procesar cada imagen, tras procesarlas todas, y antes de volver a empezar, o cada n imágenes. A dicho tamaño n, se le conoce como tamaño de batch: es el tamaño del conjunto de imágenes que han de procesarse para realizar una actualización de pesos del modelo. 

Este número es un hiperparámetro clave, ya que tiene relación directa con la convergencia: a mayor tamaño, mayor velocidad de convergencia, pero menor precisión; y el caso inverso ocurre con el caso extremo, si, por ejemplo, actualizamos con cada ejemplar procesado. En la literatura, existen gran cantidad de estudios acerca de este valor, y establecen el valor adecuado en un intervalo $[2, 32]$,  compuesto únicamente por las potencias de 2   \cite{masters2018revisiting}. Debido a limitaciones de memoria, y la demostración de su optimalidad, emplearé tamaño 32 para los 3 modelos, aunque otros tamaños, como 64 o 128, también son viables.

A partir de ahora, denominaremos época al proceso de recorrer cada uno de los batches del datablock.

\subsubsection{Definición del modelo. Transfer learning}

Con los datos correctamente definidos, podemos construir el modelo propiamente dicho para el entrenamiento. En Fastai, la construcción de modelos puede hacerse de forma modular, mediante la especificación de las capas que componen la red, ya que la librería dispone de los módulos ya configurados a excepción de los canales de entrada y salida. A pesar de ello, la creación de un modelo desde cero es muy costoso computacionalmente, ya que se parte el entrenamiento de valores aleatorios. Por este motivo, normalmente se tiende a emplear modelos preentrenados, y adaptar su salida mediante el uso de transfer learning.

El transfer-learning es una métodología que permite utilizar complejas redes convolucionadas preeentrenadas con datasets de gran tamaño que son adaptadas para utilizarse con otros datasets diferentes y obtener buenos resultados, sin necesidad de realizar un entrenamiento desde cero del modelo.\\

De forma intuitiva, el proceso a seguir es el siguiente: partimos de las capas ocultas ya entrenadas del modelo, y sustituimos su cabecera por una nueva que se adapte a nuestro problema. Por cabecera(head), entendemos la parte final de la red, punto en el cual se elabora la salida final que la red pasa a la función SoftMax para obtener las probabilidades de pertenencia a cada clase de la imagen de entrada. Como la red fue entrenada para otro dataset, es probable que el número de clases de salidas sea diferente, y por eso, debemos de sustituirla por una nueva que se adapte a las necesidades de nuestro problema.\\

Una vez sustituida el head de la red, debemos de entrenar la nueva cabecera para que sus pesos se adapten a los datos de entrada y ofrezcan buenas métricas. Pero no nos interesa modificar el resto de capas ocultas, ya que fueron entrenadas previamente y tienen valores de W adecuados. Realizaremos un "congelado" de estas capas, para únicamente entrenar la nueva adición.

Posteriormente, para afinar el comportamiento de la red completa, ``descongelaremos'' la red para hacer un entrenamiento breve a la red completa, durante un número pequeño de épocas. Así, obtemos un modelo final adaptado a nuestro problema.\\

En esta primera fase, debemos ofrecer una salida binaria, que nos ofrezca una respuesta de 0 (benigno) y 1 (maligno). Para conseguirlo, recurriremos a retirar la capa predefinida con la que viene nuestro modelo, y para todos ellos, emplearemos la misma cabecera, la cual es creada de forma automática por fastai tras especificar el número de salidas.

\begin{table}[H]
	\centering
	\caption{Cabecera para trasfer-learning binario}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Capa} & \textbf{Tam. Entrada} & \textbf{Tam. Salida} \\ \hline
		Average pooling & n x n & 32 x 2048 \\ \hline
		Flatten & 32 x 2048 & 4096 \\ \hline
		Fully connected & 4096 & 512 \\ \hline
		Fully connected & 512 & 32 x 2048 \\ \hline
		Fully connected & 32 & 2 \\ \hline
		Softmax & - & - \\ \hline
	\end{tabular}
	\label{header}
\end{table}

Dicha cabecera, observable en la figura \ref{header}, recibe como tamaño de entrada n x n, siendo n el tamaño de la salida de la última capa de la red preentrenada que estemos utilizado. Mediante una oporación de average pooling, podemos convertir dicha entrada a un tamaño de salida concreto, el cual nos permitirá tener a partir de este punto una red totalmente conectada la cual no requiere ser modificada aunque modifiquemos el tamaño de entrada de la red por imágenes de mayor o menor resolución. \\

La configuración escogida tras el flattening de la imagen, que consiste en vectorizar su contenido, es una tradicional triple capa de redes totalmente conectadas: la estructura tradicional de una red neuronal donde cada neurona se encuentra conectada a las siguientes. Dicha estructura de entradas y salida sigue forma de ``embudo'', en el que el tamaño de la capa final coincide con el  número de salidas, en este caso, dos clases.

Para instanciar el modelo ya configurado, se hace uso de la clase vision\_learner, la cual recibe como entreada el modelo, expresado como cadena, un vector con los nombres de las métricas, el datablock de entrenamiento y validación creado anteriormente, y las funciones de callback. Estas últimas son funciones que serán ejecutadas cada vez que se complete una época de nuestro modelo. Como entrada, he decidido añadir un callback de salvado, que guarde el modelo tras cada época finalizada, y un monitor de Early stoping.


\begin{figure}[H]
	\centering
	\label {earlystopping}
	\includegraphics[scale = 0.25]{imagenes/earlystopping.png}
	\caption{Demostración gráfica del Early stopping}
\end{figure}

Resumiendo, early stopping es un mecanismo de parada automática del entrenamiento, el cual, especificando una paciencia, en época, si no se cumple una condición, se detiene el modelo; en este problema, he decidido que el monitor de parada supervise el error de pérdida en validación, para así evitar el sobreaprendizaje de la red: la memorización completa del conjunto de entrenamiento, conllevando el riesgo de pérdida de generalidad. Controlaremos este fenómeno gráficamente mediante la muestra de las pérdidas de entrenamiento y validación.

\subsubsection{Entrenamiento}

El entrenamiento es la otra parte clave de la creación de modelos de deep learning. Esto se debe a la existencia de usa serie de hiperparámetros que debemos regular para asegurar el correcto entrenamiento del modelo. En este apartado, trataramos los dos parámetros clave: el número de épocas de entrenamiento, y el learning rate.

El número de épocas es equivalente al número de iteraciones realizadas sobre la totalidad de los datos del modelo, Éste debe ser fijado de forma estratégica para evitar que se produzca sobreentrenamiento, donde en lugar de aprender las características generales de los datos y su distribución, pasamos a su memorización completa. Este fenómeno, como ya se ha explicado, provoca pérdida de generalidad del modelo fuera del conjunto de entrenamiento, y no es lo deseable. En lugar de establecer dicho número de forma fija, usaremos, como ya enunciamos, Early stoping. De esta forma, será el propio modelo quien se detenga sin necesidad de que establezcamos un valor manual. Concretamente, emplearemos una paciencia de 3 épocas sobre la pérdida de validación.\\

En cuanto al learning rate, éste es un valor crítico para definir el tamaño de salto que se produce a la hora de optimizar el modelo. Define el tamaño del salto (step) que se produce a través del hiperplano definido por la función que modela los datos, en búsqueda de su mínimo local (o global). AL igual que el tamaño de batch, define de manera implícita la precisión de la búsqueda del óptimo; a mayor valor, mayor velocidad de convergencia, pero menor certeza de encontrar el óptimo de forma precisa; y a menor valor, mayor tiempo de entrenamiento, pero mayores garantías de encontrar un valor cercano al óptimo de la función.

La decisión de este parámetro normalmente se realiza mediante pruebas experimentales. Sin embargo, al disponer de recursos hardware limitados, no es posible realizar experimentos en paralelo en búsqueda de este valor, cuyo entrenamiento podría conllevar semanas. Por tanto, en lugar de usar un learning rate fijo, se ha tomado la decisión de usar un learning rate adaptativos, mediante la política de entrenamiento de un solo ciclo (Fit one cycle en FastAI) propuesta por Leslie Smith \cite{smith2018disciplined}.

Con esta técnica, el valor del learning rate se ajusta de forma iterativa, partiendo de un valor pequeño, llegando a un valor máximo, y finalizando en un valor más pequeño que el inicial.

Este proceso se realiza para conseguir una buena detección de óptimos, sin necesidad de conocer el valor exacto de learning rate, que es una tarea compleja. Este se basa en el compartamiento de la teoría de learning rate cíclicos, donde el learning rate parte de un valor pequeño, llega a un máximo, y luego se decrementa de nuevo hasta un valor mínimo, de forma periódica. Pero la diferencia radica en que fit one cycle realiza un único ciclo de crecimiento y decrecimiento del valor del learning rate.

Otra gran ventaja de este método es que no se requieren gran cantidad de parámetros. Con la implementación realizada en Fastai, solo es necesario especificar el número de épocas a realizar, y un valor máximo de learning rate, clave para especificar el valor pico de la fase ascendente del lr.

Fastai también implementa un método para ayudar en la búsqueda del learning rate pico, llamado lr\_find. Este método realiza un estudio de diferentes valores de learning rate y su capacidad minimizadora de la función de pérdida, y devuelve puntos de partida razonables para el inicio del entrenamiento: puntos pico, valle , y pendiente. De todos ellos, nos quedaremos con el punto valle, ya que es el que mejores propiedades de generalización posee.\\

La decicisión, por tanto, de elegir este método, es la considerable reducción de costo computacional que se produce, así como la mejora de los resultados, tal y como se demuestra en el paper original publicado por L. Smith \cite{smith2018disciplined}. 

\subsection{Resultados}

Una vez realizado el proceso de preparación de los modelos, comienza el período de aprendizaje. Para comparar los resultados de cada uno de los modelos, se ha recopilado información acerca de las métricas ya mencionadas, las pérdidas, y, sobre todo, el accuracy balanceado, para observar la tasa de acierto de cada uno de los modelos.\\

Destacar también que, en esta parte del proyecto, la gran cantidad de imágenes y el impacto de la calidad de las mismas provocaban problemas desbordamientos de memoria, que interrumpían el entrenamiento por falta de recursos hardware. Debido a este motivo, únicamente se ha utilizado el subconjunto de imágenes que pertenecen al dataset ISIC, el cual, engloba un aproximado 60\% de los datos totales recopilados, a un tamaño máximo de entrada de 512x512. Reservaremos las imágenes del resto de datasets para la distinción de enfermedades malignas y benignas de forma más concreta.

A continuación, mostramos los resultados obtenidos por cada modelo.

\subsubsection{ResNet 50}

Los resultados con ResNet 50 son bastante satisfactorios\ref{tab:resultsbinrn50} , ya que consiguen mejorar los ya vistos anteriormente cuando se probó el entrenamiento mediante Focal Loss.

\begin{table}[!ht]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\ \hline
		\textbf{Benign} & 0.87 & 0.91 & 0.89 & 7205 \\ \hline
		\textbf{Malignant} & 0.70 & 0.60 & 0.64 & 2460 \\ \hline
		\textbf{Accuracy} & ~ & ~ & 0.83 & 9665 \\ \hline
		\textbf{Macro avg} & 0.78 & 0.76 & 0.77 & 9665 \\ \hline
		\textbf{Weighted avg} & 0.83 & 0.83 & 0.83 & 9665 \\ \hline
	\end{tabular}
	\caption{Informe de clasificación Resnet50}
	\label{tab:resultsbinrn50}
\end{table}


Podemos apreciar que el aprendizaje de la clase minoritaria, la maligna, ha mejorado considerablemente respecto a la alternative de ajustar la función de pérdida unicamente. El aumento del accuracy se ve potenciado, sobre todo, en el aumento de la precisión y el recall. Mientras que sin hacer oversampling, obteníamos 68\%(0.68) y 41\% (0.41), respectivamente, ahora obtenemos 70 \% (0.70) y 60\% (0.60), siendo por tanto el aumento en casi 0.2 en el caso del recall. Esto significa que estamos consiguiendo mejores resultados a la hora de distinguir las lesiones malignas, y estas se diagnostican correctamente.

En cuanto a la clase benigna, hemos sacrificado parte de los resultados al realizar el equilibrado, pero sus valores siguen siendo muy buenos: entorno al 87\% de perecisión, y 91\% de recall.


Si evaluamos el modelo en cuanto a su tasa constancia en el aprendizaje, debemos examinar primero las curvas descritas por la función de perdida en entrenamiento y validación, observables en la figura \ref{fig:curvasrensetbinaria}

l\begin{figure}[H]
	\centering
	\subfigure[Fase de congelado]{\includegraphics[width=0.45\textwidth]{imagenes/bin_class_resnetfreeze.png}} 
	\subfigure[Descongelado]{\includegraphics[width=0.45\textwidth]{imagenes/bin_class_resnetunfreeze.png}} 
	\caption{Curvas del finetuning del modelo binario con Resnet50}
	\label{fig:curvasrensetbinaria}
	
\end{figure}

El aprendizaje es correcto durante la fase de congelado, pues podemos apreciar con la curva de entrenamiento y validación descienden en valor hasta la época 10, momento en el cual la pérdida asciende durante 3 épocas seguidas y provoca la interrupción por early stopping. No podemos afirmar lo mismo de la fase de descongelado, donde podemos apreciar una subida abrupta del valor de la función de pérdida de validación, lo que quiere decir que la red ya estaba correctamente entrenada y el descongelado únicamente ha provocado sobreaprendizaje.\\

En total, el modelo ha requerido 4 horas y media para el proceso de entrenamiento, y casi 1.65 horas para el proceso de oversampling de la clase minoritaria, que ha llegado a ocupar 240GB de disco.\\

Por tanto, el mecanismo de oversampling parece haber funcionado, sin apenas penalizar los resultados ya obtenidos para la clase mayoritaria, aunque con grandes requisitos de almacenamiento. Sin embargo, debemos comparar con los otros dos modelos antes de decidir cuál se empleará para cuantizar y usar de forma definitiva en la app.

\subsubsection{Efficient Net}

Efficient net B5 es la segunda arquitectura elegida para su comparación. El resultado ofrecido, siguiendo los mismos procedimientos aplicados en Resnet, son los mostrados en la tabla \ref{tab:resefnet}.

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\ \hline
		\textbf{Benign} & 0.89 & 0.93 & 0.91 & 7205 \\ \hline
		\textbf{Malignant} & 0.76 & 0.65 & 0.70 & 2460 \\ \hline
		\textbf{Accuracy} &  &  & 0.85 & 9665 \\ \hline
		\textbf{Macro avg} & 0.85 & 0.79 & 0.8 & 9665 \\ \hline
		\textbf{Weighted avg} & 0.83 & 0.83 & 0.86 & 9665 \\ \hline
	\end{tabular}
	\caption{Informe de clasificación de EfficientNet B5}
	\label{tab:resefnet}
\end{table}

A priori, podemos apreciar unos resultados ligeramente mejores al caso anterior, siendo de media, 0.02 superior en cada una de las métricas establecidas para la comparación. Esto nos deja con una tasa de acierto equilibrada de 85\% en ambas clases. Se trata, por lo tanto, del modelo con mejores resultados hasta la fecha. Sin embargo, si analizamos las curvas de aprendizaje \ref{fig:curvasefnetbinaria}, podemos observar un fenómeno llamativo: la curva no sigue la tendencia habitual a minimizarse, y posee picos de gran altura donde el error de validación se dispara.

l\begin{figure}[H]
	\centering
	\subfigure[Fase de congelado]{\includegraphics[width=0.45\textwidth]{imagenes/bin_class_efnetfreeze.png}} 
	\subfigure[Descongelado]{\includegraphics[width=0.45\textwidth]{imagenes/bin_class_efnetunfreeze.png}} 
	\caption{Curvas del finetuning del modelo binario con EfficientNet B5}
	\label{fig:curvasefnetbinaria}
\end{figure}

Este fenómeno puede deberse al reajuste de pesos fruto del learning rate variable, donde un subconjunto de las imágenes correctamente etiquetadas es cambiada de clase a la hora de modificar los valores para adaptarse a otro conjunto distinto no correctamente clasificado hasta el momento. También puede tratarse de un mal ajuste del learning rate, pero esto no es posible al utilzar la política de un ciclo con learning rate adaptativo.\\

De forma aislada, como ocurre en la fase de descongelado, este fenómeno no adquiere mayor importancia; pero, dada la periodicidad con la que lo encontramos, es preferible valorar sus resultados con cautela. En este modelo, el tiempo de aprendizaje ha sido de aproximademante 7.7 horas, pero ha requerido el reinicio del entrenamiento en 5 ocasiones debido a problemas de memoria durante el entrenamiento.

\subsubsection{Mobile Net}

Mobilenet es la red con menor número de operaciones de las 3 opciones, debido a que está destinada directamente al uso en dispositivos de baja potencia. Los resultados de esta red, como eran de esperar, son inferiores a los otros 2 modelos probados \ref{tab:mob}.   

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\ \hline
		\textbf{Benign} & 0.87 & 0.86 & 0.86 & 7205 \\ \hline
		\textbf{Malignant} & 0.60 & 0.61 & 0.60 & 2460 \\ \hline
		\textbf{Accuracy} &  &  & 0.80 & 9665 \\ \hline
		\textbf{Macro avg} & 0.73 & 0.74 & 0.73 & 9665 \\ \hline
		\textbf{Weighted avg} & 0.80& 0.80 & 0.80 & 9665 \\ \hline
	\end{tabular}
	\caption{Informe de clasificación de EfficientNet B5}
	\label{tab:mob}
\end{table}

Podemos apreciar como onde se obtienen los peores resultados es en la columna de precisión, donde la diferencia con los modelos anteriores es de 0.11 y 0.16, respectivamente. Tratándose de la clase minoritaria, es un factor clave que este valor sea lo más alto posible, por lo que el redimiento no es el deseado. Sin embargo, resulta una opción atractiva a la hora de ejecutar el modelo dispositivos de baja potencia.

En cuanto a las curvas de aprendizaje (figura \ref{fig:curvasmbin}), se puede apreciar un comportamiento prácticamente ideal de la función  de pérdida en su época de descongelado; tanto el estimador de la pérdida en la función de entrenamiento como el de validación progresan en un intervalo de valores muy similares, sin picos bruscos. Pero, se puede observar cierto aplanamiento del modelo, lo cual puede indicar que hemos alcanzado la capacidad de la red, y sería necesario un modelo más profundo para minimizar aún más la pérdida. En la fase de descongelado, se puede apreciar que no se realizó un apredizaje adecuado, y que la tendencia de validación es al alza, por lo que se trata de un caso de sobreentrenamiento.

l\begin{figure}[H]
	\centering
	\subfigure[Fase de congelado]{\includegraphics[width=0.45\textwidth]{imagenes/perdidasmobv2_freeze_equilib.png}} 
	\subfigure[Descongelado]{\includegraphics[width=0.45\textwidth]{imagenes/perdidasmobv2_unfreeze_equilib.png}} 
	\caption{Curvas del finetuning del modelo binario con Mobilenet V2}
	\label{fig:curvasmbin}
\end{figure}

En resumen, se trata de uno de los modelos más eficientes,    debido a su estructura especialmente optimizada, y requiriendo únicamente 2.26 horas de entrenamiento, pero se trata de un modelo con capacidades limitadas para el problema que estamos tratando.

\subsection{Conclusión: modelo seleccionado}

Atendiendo a los modelos entrenados, considero justificada la elección de Resnet50 para la generación del modelo cuantizado. Aunque se trata del segundo mejor modelo en lo que a accuracy balanceado se refiere, EfficientNet ha provocado demasiados problemas a nivel local, desde errores de memoria disponible de CUDA, hasta el comportamiento exhibido en el punto anterior, donde se producían picos periódicos durante el entrenamiento.

MobileNet es el peor modelo de los 3 en todas las métricas observadas, pero con la ventaja de ofrecer un menor tiempo de inferencia. Por tanto, considero suficientemente justificada la decisión de usar ResNet50 para clasificación binaria, y las pruebas que restan de los modelos especializados. En base a validación, y al comportamiento de la curva de aprendizaje, se trata del modelo más equilibrado.

\begin{figure}[H]
	\centering
	\label {fig:metricas}
	\includegraphics[scale = 0.35]{imagenes/metricas.png}
	\caption{Valores de las métricas de cada modelo}
\end{figure}

Si bien Resnet ofrece un valor cercano, el tiempo de inferencia ofrecido por este es menor. Además, en cuanto a tamaño, EfficientNet posee un número de parámetros excesivo, que provoca una necesidad de almacenamiento elevada. (figura \ref)
\begin{figure}[H]
	\centering
	\label {fig:tam}
	\includegraphics[scale = 0.6]{imagenes/tamestimado.png}
	\caption{Espacio (en MB) requerido por cada modelo}
\end{figure}

Por tanto, realizaremos el proceso de cuantizado con Resnet, ya que se tratará de un modelo más versátil para la clasificación multiclase.

\subsection{Cuantización}

Pytorch contiene un proceso de cuantización directo a partir de los modelos entrenados con FastAI. Proporciona una interfaz de métodos ya preconfigurada que permite exportar el modelo de forma directa a Android, o bien, haciendo uso de cuantización.

La optimización aplicada al realizar cuantización se centra, sobre todo, en la reducción de la precisión numérica del modelo, pero cubre los siguientes aspectos:

\begin{itemize}
	\item Fusión de las capa de convolución y normalización de batch: realiza una integración de los valores de normalización dentro de la capa convolutiva, ya que no será necesario reajustar esos valores para inferencia-
	\item Inserción y plegado de operaciones: utiliza la librería XNNPack, que se trata de un sistema de operaciones en coma flotante especialmente optimizado para ARM, que permite variar la precisión de los valores flotantes según el modelo. Fusiona las operaciones lineales y de convolución, de forma que se realicen en un menor tiempo.
	\item Fusión de la función ReLu con el conjunto de operaciones empaquetado creado con XNNPack en el paso anterior.
	\item Eliminación del dropout, ya que en tiempo de inferencia, se busca el mejor resultado si necesidad de reentreno.
	\item Optimización del grafo interno del modelo correspondiente a la convolución, para hacer que formen parte de un solo bloque raiz y mejore el tiempo de inferencia.
	
En total, la web estima una ganancia del 60\% en tiempo de inferencia de forma teórica. Examinaremos este resultado de forma práctica empleando el modelo elegido, y comparando el funcionamiento de versión optimizada con la original para verificarlo.

\subsubsection{Creación del modelo cuantizado}
\subsubsection{Evaluación de modelo original y cuantizado}
	
\end{itemize}


\section {Clasificación multiclase}

Una vez distinguida si la enfermedad de entrada es benigna o maligna, debemos de indicar de qué tipo de enfermedad se trata. Para ello, crearemos dos modelos especializados: uno en imágenes benignas, y otro especializado en malignas. De esta forma, favorecemos la especialización de modelos, y facilitamos el proceso de entrenamiento al mostrar imágenes dentro del intervalo de tamaño de cada clase; es decir, como la clase benigna es mayoritaria, si evaluamos sus subtipos frente a los malignos, los benignos seguirán teniendo sesgo a favor. Al separarlo en dos, esto podrá evitarse, con el único inconveniente de necesitar un extra de espacio en disco.

El proceso a seguir será exactamente el mismo que con clasificación binaria, pero con ajustes en la cabecera de la red para dar salida a tantas clases de salida como tipos de entrada existen.

\subsection{Modelo de enfermedades malignas}

Comenzaremos por las imágenes malignas. En este caso, al estar utilizando finalmente únicamente el conjunto de entrenamiento de ISIC, disponemos de 4 clases, con un desbalanceo considerable. La clase menos representada se trata de melanoma metastasis. Debido a que su número es muy inferior al del resto de clases,  representando solo el 0.22 \% del total de datos malignos, por riesgo a una incorrecta clasificación de esta enfermedad,  he decidido fusionar dicha clase con melanoma (figura \ref{fig:malas}). Aunque aumentar la clase mayoritaria añadiendo este conjunto de imágenes puede parecer negativo, al tratarse de la misma enfermedad en una fase más avanzada, conseguimos aumentar la variabilidad de las imágenes.

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.6]{imagenes/countmalignant.png}
	\caption{Conteo de clases malignas antes de agrupar}
	\label {fig:malas}
\end{figure}

\subsubsection{Entrenamiento}

Para el entrenamiento del modelo, hare uso de la misma configuración que la clasificación binaria: imágenes 512 x 512, tamaño de batch de 32, pero con la diferencia de no emplear oversampling, para hacer el modelo lo más fiel posible al conjunto de imágenes. Además, tras la fusión de la clase minoritaria, el desequilibrio entre clases es razonable, siendo de:

\begin{itemize}
	\item Melanoma: 53.95\%
	\item Células basales: 36\%
	\item Células escamosas: 10.5\%
 \end{itemize}
 
 Tras el entrenamiento, se han obtenido las métricas referenciadas en la tabla \ref{tab:malignometrics}. Se puede observar como los resultados son positivos, ya que se ha conseguido alcanzar un valor de accuracy cercano al 84\% de forma balanceada.  
 
\begin{table}[!ht]
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		& Precision & Recall & F1-score & Support \\
		\hline
		Basal cell & 0.81 & 0.85 & 0.83 & 908 \\
		Melanoma & 0.88 & 0.92 & 0.90 & 1311 \\
		Squamous cell & 0.66 & 0.38 & 0.48 & 240 \\
		\hline
		Accuracy &  &  & 0.84 & 19464 \\
		Macro avg & 0.79& 0.72& 0.74&2459\\
		Weighted avg&0.84&0.84&0.84&2459\\
		\hline
	\end{tabular}
	\caption{Informe de clasificación para validación maligna}
	\label{tab:malignometrics}
\end{table}

La precisión del modelo también es considerablemente buena, ya que alcanza el 74\%. Son resultados satisfactorios tratándose de un conjunto de datos desbalanceado, y con las limitaciones de tamaño del modelo requeridas.

\subsubsection{Cuantización}


\subsection{Modelo de enfermedades benignas}

Tras evaluar los resultados obtenidos en clasificación maligna, procedemos al caso dual: las enfermedades benignas, o no cancerosas. En este caso, disponemos de un conjunto de datos mucho más amplio, englobando 17 clases distintas. En este caso, el desbalanceo que se produce es un caso límite, en cual la clase mayoritaria, el lunar común, engloba el 81\% del total. Se probarán dos alternativas distintas: la aplicación de la métrica Focal Loss, y la utilización de imágenes complementarias provenientes de los datasets distintos al ISIC.

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.6]{imagenes/countbenign.png}
	\caption{Conteo de clases malignas antes de agrupar}
	\label {fig:buenas}
\end{figure}

\subsubsection{Entrenamiento}

Conocemos que el desequilibrio del conjunto de entrenamiento es considerable. Pero, antes de probar otras soluciones más sofisticadas, es razonable considerar aplicar la misma técnica que la aplicada en clasificación binaria: el uso de la una métrica que tolere adecuadamente el desbalanceo, como FocalLoss.\\

 
\begin{table}[!ht]
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		& Precision & Recall & F1-score & Support \\
		\hline
		       \textbf{NV} & 0.92 & 0.98 & 0.95 & 5864 \\ \hline
		       \textbf{SK} & 0.50 & 0.35 & 0.41 & 345 \\ \hline
		       \textbf{AK} & 0.53 & 0.58 & 0.56 & 265 \\ \hline
		       \textbf{PBK} & 0.71 & 0.74 & 0.72 & 222 \\ \hline
		       \textbf{SL} & 0.28 & 0.12 & 0.17 & 99 \\ \hline
		       \textbf{DER} & 0.75 & 0.18 & 0.29 & 83 \\ \hline
		       \textbf{VL} & 0.68 & 0.55 & 0.61 & 71 \\ \hline
		       \textbf{LK} & 0.67 & 0.07 & 0.12 & 61 \\ \hline
		       \textbf{ACR} & 0.46 & 0.24 & 0.32 & 54 \\ \hline
		       \textbf{LN} & 0.16 & 0.11 & 0.13 & 44 \\ \hline
		       \textbf{AMP} & 0.50 & 0.07 & 0.12 & 29 \\ \hline
		       \textbf{AIMP} & 0.00 & 0.00 & 0.00 & 24 \\ \hline
		       \textbf{WART} & 0.00 & 0.00 & 0.00 & 23 \\ \hline
		       \textbf{ANG} & 0.29 & 0.33 & 0.31 & 6 \\ \hline
		       \textbf{LS} & 0.00 & 0.00 & 0.00 & 5 \\ \hline
		       \textbf{NEU} & 0.00 & 0.00 & 0.00 & 5 \\ \hline
		       \textbf{SCAR} & 0.00 & 0.00 & 0.00 & 5 \\ \hline
		\hline
		Accuracy &  &  & 0.87 & 7205 \\
		Macro avg & 0.38& 0.25& 0.28&7205\\
		Weighted avg&0.84&0.87&0.85&7205\\
		\hline
	\end{tabular}
	\caption{Informe de clasificación para validación benigna (Ordenadas de mayor a menor rep.)}
	\label{tab:benignomalmetrics}
\end{table}

El resultado obtenido deja bastante que desear  (\ref{tab:benignomalmetrics}), debido a que la clase de lunares comunes es la única que ha recibido resultados excelentes; el cuanto a precisión, sin tener en cuenta al lunar común,  los mejores resultados provienen de los dermatofibromas, queratosis benigna pigmenteada,  las lesiones vasculares, y la queratosis liquenoide. Todas ellas, obtienen resultados razonables, aunque a partir de estas clases, los resultados empeoran considerable.

Se ve mayormente reflejado en las tres últimas clases: la cicatrices, los neurofibromas y los lentigo simples. Estas clases, al encontrarse en absoluta minoría, no aportan suficientes caracteristicas al modelo para aprender los filtros adecuados, y por tanto, quedan descartadas por el mismo. Esto nos lleva a tomar la decisión de descartar estas clases, o bien, tratar de equilibrarlas complementando con el resto de datasets.

No nos debemos dejar llevar por el valor del accuracy: este es del 87\%, debido a que prácticamente la completitud de la tasa de aciertos se ve aportada por la clase mayoritaria. Si atendemos a su valor medio, es del únicamente 28\%, que se trata de una cifra más representativa.\\

Antes de probar la eliminación de clases minoritarias, trataremos de dotar al modelo de penalizaciones para las clases menos reprentadas; es decir, de forma explícita, estableceremos una serie de pesos que otorguen más importancia a clasificar ejemplares de clases minoritarias sobre las mayoritarias, calculando para ello un peso inversamente proporcional al recuento de ejemplares. Basta con aplicar la siguiente operación, teniendo en cuenta el valor c como el número de elementos de la clase C:

$$p_C =(\frac{c}{\sum_{i=0}^{num\_clases} c} )^{-1}$$

De esta forma, haremos que cada ejemplar tenga un peso acorde a su proproción en el dataset. Si repetimos el experimento siguiendo este razonamiento, obtenemos los resultados de la tabla \ref{tab:benignomalmejormetrics.}


\begin{table}[!ht]
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		& Precision & Recall & F1-score & Support \\
		\hline
	\textbf{NV} & 0.99 & 0.77 & 0.86 & 5864 \\ \hline
	\textbf{DER} & 0.33 & 0.60 & 0.42 & 83 \\ \hline
	\textbf{PBK} & 0.53 & 0.59 & 0.56 & 222 \\ \hline
	\textbf{VL} & 0.37 & 0.88 & 0.52 & 71 \\ \hline
	\textbf{LK} & 0.18 & 0.46 & 0.26 & 61 \\ \hline
	\textbf{AK} & 0.31 & 0.63 & 0.41 & 265 \\ \hline
	\textbf{SK} & 0.58 & 0.83 & 0.69 & 345 \\ \hline
	\textbf{AMP} & 0.20 & 0.39 & 0.26 & 29 \\ \hline
	\textbf{ACR} & 0.23 & 0.67 & 0.34 & 54 \\ \hline
	\textbf{ANG} & 0.12 & 0.43 & 0.19 & 6 \\ \hline
	\textbf{SL} & 0.23 & 0.48 & 0.31 & 99 \\ \hline
	\textbf{LN} & 0.01 & 0.04 & 0.01 & 44 \\ \hline
	\textbf{AIMP} & 0.40 & 0.26 & 0.32 & 24 \\ \hline
	\textbf{WART} & 0.18 & 0.67 & 0.29 & 23 \\ \hline
	\textbf{LS} & 0.06 & 0.20 & 0.09 & 5 \\ \hline
	\textbf{NEU} & 0.00 & 0.00 & 0.00 & 5 \\ \hline
	\textbf{SCAR} & 0.00 & 0.00 & 0.00 & 5 \\ \hline
		\hline
		Accuracy &  &  & 0.74 & 7205 \\
		Macro avg & 0.28& 0.46& 0.33&7205\\
		Weighted avg&0.87&0.74&0.78&7205\\
		\hline
	\end{tabular}
	\caption{Informe de clasificación para validación benigna (Ordenadas de mayor a menor rep.)}
	\label{tab:benignomalmejormetrics}
\end{table}

Los resultados han mejorado, escalando hasta el 46\% de accuracy balanceado. Sin embargo, estos siguen siendo pobres, y las clases minoritarias siguen siendo un problema. Por tanto, las 3 clases de menor representación sería oportuno eliminarlas, debido a la imposibilidad de unir estas enfermedades a otras clases existenetes. En el caso de los lentigo, que se encuentran divididos en 3 clases, podemos unirlos en un una única clase para mejorar los resultados.

\subsubsection{Cuantización}

